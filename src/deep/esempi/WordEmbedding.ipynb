{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Feb 23 10:53:10 2022\n",
    "\n",
    "@author: Luigi Portinale\n",
    "\"\"\"\n",
    "from numpy import array\n",
    "from keras.api.preprocessing.sequence import pad_sequences\n",
    "from keras.api.models import Sequential\n",
    "from keras.api.layers import Dense\n",
    "from keras.api.layers import Flatten\n",
    "from keras.api.layers import Embedding\n",
    "from keras.api.layers import Input\n",
    "#from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "# define class labels (1: positive; 0: negative)\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "\n",
    "def custom_one_hot(doc, vocab_size):\n",
    "    words = doc.split()\n",
    "    encoded = [hash(word) % vocab_size for word in words]\n",
    "    return encoded\n",
    "\n",
    "# integer encode the documents\n",
    "#keras tensorflow one_hot(d,s) is a hash function returning an integer \n",
    "#for each word d; it uses a vocabulary size s that should be the number\n",
    "#of possible words to hash. If this is greater than the actual vocabulary\n",
    "#size then the probability of collisions is reduced\n",
    "vocab_size = 50\n",
    "encoded_docs = [custom_one_hot(d, vocab_size) for d in docs]\n",
    "print(\"The encoded documents:\")\n",
    "print(encoded_docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# each document is then a vector of 4 integers!\n",
    "print(\"The padded documents:\")\n",
    "print(padded_docs)\n",
    "\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "\n",
    "#The Embedding has a vocabulary of 50 and an input length of 4. \n",
    "#We will choose a small embedding space of 8 dimensions.\n",
    "#Embedding needs: the vocabulary size, \n",
    "#the embedding output dimension (each word will be embedded in a vector\n",
    "#with this dimension) that is the output size of the layer and \n",
    "#finally the length of each document that\n",
    "#will be provided in input (input size of the layer)\n",
    "model.add(Input(shape=(max_length,)))\n",
    "model.add(Embedding(vocab_size, 8))\n",
    "# each document in input at the above layer will have a vector\n",
    "#of 8 integers for every word in the document\n",
    "\n",
    "#now we flatten the output of the embeddings\n",
    "model.add(Flatten())\n",
    "#then we use an outout layer for binary classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=100, verbose=1)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
